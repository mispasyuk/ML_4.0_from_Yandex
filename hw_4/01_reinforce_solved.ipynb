{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVkCC1iri2SN"
      },
      "source": [
        "## HW 4: Policy gradient\n",
        "_Reference: based on Practical RL course by YSDA_\n",
        "\n",
        "In this notebook you have to master Policy gradient Q-learning and apply it to familiar (and not so familiar) RL problems once again.\n",
        "\n",
        "To get used to `gymnasium` package, please, refer to the [documentation](https://gymnasium.farama.org/introduction/basic_usage/).\n",
        "\n",
        "\n",
        "In the end of the notebook, please, copy the functions you have implemented to the template file and submit it to the Contest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "7UYczVTli2Sb"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "XPKYrIlai2Sf",
        "outputId": "1d7940fd-b0d5-4911-a2a7-3f0c538eff66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7d2f86c92150>"
            ]
          },
          "metadata": {},
          "execution_count": 300
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75eHkuwTi2Si"
      },
      "source": [
        "# Building the network for Policy Gradient (REINFORCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_TFCmsWi2Sj"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "id": "sY2THBWfi2Sl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_features, output_features):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_features, input_features*2)\n",
        "    self.fc2 = nn.Linear(input_features*2, output_features)\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "d7UBkpo38sNw"
      },
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "id": "8_pYr7PZi2Sn"
      },
      "outputs": [],
      "source": [
        "# Build a simple neural network that predicts policy logits.\n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "n_features = len(env.reset()[0])\n",
        "model = MLP(n_features, n_actions)\n",
        "assert model is not None, \"model is not defined\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([env.reset()[0] for _ in range(5)])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F-uVfkvA-n9",
        "outputId": "239504a4-60a4-47f2-96f1-2294e04a5010"
      },
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.04350946, -0.0219835 ,  0.00441534,  0.02607844],\n",
              "       [-0.02863725, -0.04839468, -0.00261779,  0.01109212],\n",
              "       [-0.00782961,  0.04678357,  0.0351448 ,  0.02101909],\n",
              "       [-0.03336489, -0.02725059, -0.04206685,  0.03527444],\n",
              "       [-0.02636818, -0.01773062, -0.04797324, -0.01835839]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.from_numpy(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5grTPbHBNz9",
        "outputId": "a8c0f935-d9f2-4730-90bb-7e71a9d8b2e3"
      },
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0435, -0.0220,  0.0044,  0.0261],\n",
              "        [-0.0286, -0.0484, -0.0026,  0.0111],\n",
              "        [-0.0078,  0.0468,  0.0351,  0.0210],\n",
              "        [-0.0334, -0.0273, -0.0421,  0.0353],\n",
              "        [-0.0264, -0.0177, -0.0480, -0.0184]])"
            ]
          },
          "metadata": {},
          "execution_count": 305
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "id": "JRYQVUUV1jeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917e1ddb-39f4-465b-94a6-ef6c8193ae86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example_states_batch.shape: (5, 4)\n",
            "example_logits.shape: torch.Size([5, 2])\n"
          ]
        }
      ],
      "source": [
        "# do not change the code block below\n",
        "batch_size_for_test = 5\n",
        "example_states_batch = np.array([env.reset()[0] for _ in range(5)])\n",
        "print(f\"example_states_batch.shape: {example_states_batch.shape}\")\n",
        "assert example_states_batch.shape == (batch_size_for_test, state_dim[0])\n",
        "\n",
        "example_logits = model(torch.from_numpy(example_states_batch))\n",
        "print(f\"example_logits.shape: {example_logits.shape}\")\n",
        "assert example_logits.shape == (batch_size_for_test, n_actions)\n",
        "# do not change the code block above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y80qbQFi2Sq"
      },
      "source": [
        "#### Predicting the action probas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12PjRu0mi2Sr"
      },
      "source": [
        "Note: **output value of this function is not a torch tensor, it's a numpy array.**\n",
        "\n",
        "So, here gradient calculation is not needed.\n",
        "\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "\n",
        "Also, `.detach()` can be used instead, but there is a difference:\n",
        "\n",
        "* With `.detach()` computational graph is built but then disconnected from a particular tensor, so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "* In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "id": "d5B5JuXCi2St"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "def predict_probs(states, model):\n",
        "    \"\"\"\n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :param model: torch model\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "\n",
        "    # YOUR CODE GOES HERE\n",
        "    states = torch.from_numpy(states)\n",
        "    with torch.no_grad():\n",
        "        logits = model(states)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    return probs.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "id": "Obkl_jCii2Sv"
      },
      "outputs": [],
      "source": [
        "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
        "test_probas = predict_probs(test_states, model)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be6AYf8gi2Sw"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "id": "8LOUUvnki2Sx"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\"\n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s, info = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]), model)[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "        new_s, r, done, truncated, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "5sdENWJAi2Sz"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGu5XGprJJHG",
        "outputId": "61142550-daae-415a-b9d8-a4fef2a58aa0"
      },
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([ 0.00122582,  0.01454615, -0.04805082, -0.04102281], dtype=float32),\n",
              " array([ 0.00151675,  0.21032304, -0.04887128, -0.3484707 ], dtype=float32),\n",
              " array([ 0.00572321,  0.01592901, -0.05584069, -0.07159023], dtype=float32),\n",
              " array([ 0.00604179, -0.17834976, -0.05727249,  0.20296507], dtype=float32),\n",
              " array([ 0.00247479,  0.01754249, -0.05321319, -0.10722045], dtype=float32),\n",
              " array([ 0.00282564, -0.17677811, -0.0553576 ,  0.16821058], dtype=float32),\n",
              " array([-0.00070992, -0.3710658 , -0.05199339,  0.44292882], dtype=float32),\n",
              " array([-0.00813124, -0.56541497, -0.04313482,  0.71877944], dtype=float32),\n",
              " array([-0.01943954, -0.36972353, -0.02875922,  0.41283765], dtype=float32),\n",
              " array([-0.02683401, -0.17420596, -0.02050247,  0.11122853], dtype=float32),\n",
              " array([-0.03031812, -0.3690282 , -0.0182779 ,  0.3973732 ], dtype=float32),\n",
              " array([-0.03769869, -0.56388617, -0.01033044,  0.68423784], dtype=float32),\n",
              " array([-0.04897641, -0.75886315,  0.00335432,  0.97365063], dtype=float32),\n",
              " array([-0.06415367, -0.9540299 ,  0.02282733,  1.2673854 ], dtype=float32),\n",
              " array([-0.08323427, -1.1494359 ,  0.04817504,  1.5671287 ], dtype=float32),\n",
              " array([-0.10622299, -0.9549214 ,  0.07951761,  1.2898535 ], dtype=float32),\n",
              " array([-0.12532142, -1.1509595 ,  0.10531468,  1.6063349 ], dtype=float32),\n",
              " array([-0.14834061, -1.3471575 ,  0.13744138,  1.9299066 ], dtype=float32),\n",
              " array([-0.17528376, -1.1537497 ,  0.17603952,  1.6828114 ], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 311
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VddxnyyHJKqq",
        "outputId": "8350d0d6-7d67-4533-edd7-f492560e86d1"
      },
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 312
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1Hn6NY6JMOO",
        "outputId": "b8d7d56e-62a1-46cf-fa28-370601f70643"
      },
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 313
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG5hLg-3i2S0"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "To work with sequential environments we need the cumulative discounted reward for known for every state. To compute it we can **roll back** from the end of the session to the beginning and compute the discounted cumulative reward as following:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "id": "AoWX9gvai2S0"
      },
      "outputs": [],
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session\n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "\n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    # YOUR CODE GOES HERE\n",
        "    if len(rewards) == 0:\n",
        "      return []\n",
        "    else:\n",
        "      t = len(rewards) - 1\n",
        "      G = [0 for i in range (len(rewards))]\n",
        "      G[t] = rewards[t]\n",
        "      while t > 0:\n",
        "        G[t-1] = rewards[t-1] + gamma*G[t]\n",
        "        t -= 1\n",
        "      cumulative_rewards = G\n",
        "      assert cumulative_rewards is not None, \"cumulative_rewards is not defined\"\n",
        "\n",
        "      return cumulative_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "id": "2DX39wcUi2S3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d0c11e4-882d-4361-cb42-87f792331052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looks good!\n"
          ]
        }
      ],
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evLt5DJji2S_"
      },
      "source": [
        "### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient.\n",
        "\n",
        "Final loss should also include the entropy regularization term $H(\\pi_\\theta (a_i \\mid s_i))$ to enforce the exploration:\n",
        "\n",
        "$$\n",
        "L = -\\hat J(\\theta) - \\lambda H(\\pi_\\theta (a_i \\mid s_i)),\n",
        "$$\n",
        "where $\\lambda$ is the `entropy_coef`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pboqcG8i1jeF"
      },
      "source": [
        "This function might be useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "id": "_hLjxTVLi2TB"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[1, 2], [3, 4]])\n",
        "b = np.array([[1, 2], [3, 4]])\n",
        "a = torch.from_numpy(a)\n",
        "b = torch.from_numpy(b)"
      ],
      "metadata": {
        "id": "ASC6kUAHnbMm"
      },
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMfp4KR5n1Zp",
        "outputId": "4b60a91d-8328-41b4-8fec-56efae817b9f"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 318
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9OhOD6Sn2OH",
        "outputId": "7c598b1a-cbca-444b-eca5-1b1ce78c481d"
      },
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a*b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uiszQnrodIs",
        "outputId": "99fbd4f2-e798-4661-f527-7d893f48d6a5"
      },
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  4],\n",
              "        [ 9, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 320
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(a*b, dim =1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbETBFgDn3V1",
        "outputId": "542f5ba6-1d15-41ef-e0e8-92a0f9f62041"
      },
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5, 25])"
            ]
          },
          "metadata": {},
          "execution_count": 321
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Compute the loss for the REINFORCE algorithm.\n",
        "    \"\"\"\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "    #print(f\"actions {actions}\\n\")\n",
        "    #print(f\"logits {logits}\\n\")\n",
        "\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    #print(f\"probs {probs}\\n\")\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    log_probs = torch.log(probs)\n",
        "    #print(f\"log_probs {log_probs}\\n\")\n",
        "    assert log_probs is not None, \"log_probs is not defined\"\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = [log_probs[i][actions[i]] for i in range (len(actions))]\n",
        "    #print(f\"log_probs_for_action {log_probs_for_actions}\\n\")\n",
        "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
        "    J_hat = sum([log_probs_for_actions[i] * cumulative_returns[i] for i in range(len(actions))]) / len(actions)\n",
        "    assert J_hat is not None, \"J_hat is not defined\"\n",
        "\n",
        "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
        "    entropy = -(torch.sum(probs*log_probs, dim = 1)).mean()\n",
        "    assert entropy is not None, \"entropy is not defined\"\n",
        "    loss = -J_hat - entropy_coef*entropy\n",
        "    assert loss is not None, \"loss is not defined\"\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "c5dWvh_XE4uk"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "id": "1C8ZSizji2TD"
      },
      "outputs": [],
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "model = MLP(4, 2)\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    logits = model(states)\n",
        "    # cast everything into torch tensors\n",
        "    loss = get_loss(logits, actions, rewards, n_actions=n_actions, gamma=gamma, entropy_coef=entropy_coef)\n",
        "    # Gradient descent step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-WWsbl5i2TE"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_features, output_features):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_features, input_features*2)\n",
        "    self.ac1 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(input_features*2, output_features)\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.ac1(x)\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Cc2Jcx-CNixx"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "id": "ckHj5sXBi2TE",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "a066af6b-3aef-4cb8-9054-3058e6f3f0c9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-841314614.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_on_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_coef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean reward:%.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1071083495.py\u001b[0m in \u001b[0;36mtrain_on_session\u001b[0;34m(states, actions, rewards, gamma, entropy_coef)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_coef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentropy_coef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Gradient descent step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "for i in range(500):\n",
        "    rewards = [train_on_session(*generate_session(env), entropy_coef=1e-3) for _ in range(100)]  # generate new sessions\n",
        "\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > 800:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg__sQeti2TF"
      },
      "source": [
        "\n",
        "### Watch the video of your results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "y67NSQk_1jeG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium.utils.save_video import save_video\n",
        "\n",
        "env_for_video = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
        "n_actions = env_for_video.action_space.n\n",
        "\n",
        "episode_index = 0\n",
        "step_starting_index = 0\n",
        "\n",
        "obs, info = env_for_video.reset()\n",
        "\n",
        "for step_index in range(800):\n",
        "    probs = predict_probs(np.array([obs]), model)[0]\n",
        "    action = np.random.choice(n_actions, p=probs)\n",
        "\n",
        "    obs, reward, terminated, truncated, info = env_for_video.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    if done or step_index == 799:\n",
        "        # env_for_video.render() now returns the LIST of frames accumulated so far\n",
        "        frames = env_for_video.render()\n",
        "        os.makedirs(\"videos\", exist_ok=True)\n",
        "        save_video(\n",
        "            frames, \"videos\",\n",
        "            fps=env_for_video.metadata.get(\"render_fps\", 30),\n",
        "            step_starting_index=step_starting_index,\n",
        "            episode_index=episode_index,\n",
        "        )\n",
        "        episode_index += 1\n",
        "        step_starting_index = step_index + 1\n",
        "        obs, info = env_for_video.reset()\n",
        "\n",
        "env_for_video.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUxpSDAi1jeG"
      },
      "source": [
        "Congratulations! Finally, copy the `predict_probs`, `get_cumulative_rewards` and `get_loss` to the template and submit them to the Contest.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS27n0mk1jeG"
      },
      "source": [
        "## Bonus part (no points, just for the interested ones)\n",
        "\n",
        "Try solving the `Acrobot-v1` environment. It is more complex than regular `CartPole-v1`, so the default Policy Gradient (REINFORCE) algorithm might not work. Maybe the baseline idea could help...\n",
        "\n",
        "![Acrobot](https://gymnasium.farama.org/_images/acrobot.gif)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-xOb1-a1jeG"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT6t6zRg1jeG"
      },
      "outputs": [],
      "source": [
        "# Your brave and victorious code here."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3_main",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}